# Agentic Delivery Repo Topology

**How to rewire repositories for AIâ€‘asâ€‘labor delivery agents (Genâ€‘4 SDLC)**
Barton Mathis

---

## The Problem

Our repo structure was designed for humans collaborating on quarterly releases.

Now we're running agents that ship features daily.

**This is like running a Formula 1 car on roads designed for horses.**

The car is fast. The road wasn't built for that speed. Something breaks.

---

## The Solution (TL;DR)

- Default to an **agentâ€‘native product monorepo** with enforced module boundaries (not suggestions).
- Pair it with a **separate controlâ€‘plane repo** for policies, golden workflows, and scaffolding.
- Optimize for **deterministic signal**: hermetic builds, "affected" CI, and aggressive caching.
- Treat PRs as **governance evidence bundles** (intent â†’ diff â†’ checks â†’ attestations), not collaboration forums.

---

## What Changes in Genâ€‘4 (AIâ€‘asâ€‘Labor)

### Traditional (Genâ€‘3)

- **Humans produce code** â€” 10-50 lines/hour, context-switching every 23 minutes
- **PRs are collaboration forums** â€” "Can you explain this?" "Why did you...?" "LGTM ğŸš€"
- **Repo structure optimized for teams** â€” "Frontend in one repo, backend in another"
- **CI is supporting infrastructure** â€” "The build is red again, someone look at it"

### Agentic (Genâ€‘4)

- **Agents produce featureâ€‘scale payloads** â€” 500-2000 lines in 15 minutes, zero context switching
- **Humans govern safety and outcomes** â€” "Ship the feature" or "Roll it back" (not "move this function to line 47")
- **Repo structure optimized for agents** â€” "Everything this agent needs is in one atomic boundary"
- **CI becomes the trust factory** â€” "If CI is green, we ship. If CI is red, nobody moves."

> ğŸ’¡ **The Shift:** Humans used to write code and delegate review. Now humans delegate coding and review outcomes.

---

## Goal: Max Throughput Without Losing Trust

**The Formula:** Agent velocity Ã— Human confidence = Sustainable delivery at scale

### What We're Optimizing For

- **Minimize coordination overhead** â€” 6 repos Ã— 3 teams = 18 handoffs per feature â†’ 1 repo Ã— atomic merge = 0 handoffs
- **Maximize agent situational awareness** â€” "Where's the auth code?" â†’ 3 grep results, not 3 repo searches
- **Make verification cheap and fast** â€” 4-hour CI runs â†’ 8-minute affected tests with caching
- **Scale governance** â€” 22 manual approval gates â†’ 4 human checkpoints + automated evidence

> âš¡ **Remember:** If our CI is flaky, our agents are flying blind. If our builds are slow, our agents are stuck in traffic.

---

## Topology Decision: Monorepo vs Multiâ€‘Repo

> âš ï¸ **War Story: The 6-Hour Feature**
>
> A SaaS company had 18 microservice repos. To ship a feature touching 3 services:
> - Day 1: Open PR in repo A, wait for CI (45 min), wait for review (4 hours)
> - Day 2: Open PR in repo B, discover contract mismatch, go back to repo A
> - Day 3: Coordinate deploy order, staging fails, debug across repos
>
> **Result:** 6 hours of agent work, 3 days of human coordination, 2 rollbacks
>
> After monorepo migration: 45 minutes, 1 atomic PR, 0 coordination overhead

### Monorepo (Default for 80% of Product Teams)

- **Atomic crossâ€‘module changes** â€” Change API + all 7 call sites in one PR, not 8 coordinated PRs
- **Shared patterns are local** â€” `import { validateEmail } from '@libs/validation'`, not "which version of the validation package?"
- **Agent navigation is grep, not GitHub search** â€” "Where's the auth middleware?" â†’ One `rg` command
- **One CI pipeline to rule them all** â€” Unified standards, consistent tooling, shared cache

### Multiâ€‘Repo (When You Actually Need It)

- **Hard access boundaries** â€” PCI-regulated payment processing physically separated from marketing site
- **Truly independent products** â€” Mobile app with 6-month release cycles + web app with daily deploys
- **Regulatory/compliance mandates** â€” "The auditor said these must be separate"
- **Organizational constraints** â€” Acquired company not ready to merge (yet)

> ğŸ¯ **Decision Rule:** If our agents touch >1 repo for >30% of features, we need a monorepo.

---

## Recommended Baseline

### Agentâ€‘Native Product Monorepo (Default)

```

/apps

*   deployables
*   service boundaries
*   release units

/libs

*   shared modules
*   public APIs
*   versionâ€‘less internal dependencies

/platform

*   standards
*   SDKs
*   runtime contracts

/infra

*   pipelines
*   environment templates
*   infrastructureâ€‘asâ€‘code

/docs

*   ADRs
*   intent specs
*   agent context

```

### Separate Controlâ€‘Plane Repo

- Policies
- Golden workflows
- Scaffolding
- Consumed by product repos

---

## Nonâ€‘Negotiables for Agentâ€‘Native Repos
*(If we skip these, agents will wreck our codebase)*

### 1. Module Boundaries Are **Enforced**, Not Social
- **Wrong:** "Please don't import from `/internal`" (agents will ignore this)
- **Right:** ESLint rules that fail CI if you import from `/internal`
- **Why:** Agents don't read comments. They follow signals.

### 2. Ownership Is Codified
- **Wrong:** "Sarah reviews backend stuff" (tribal knowledge)
- **Right:** `CODEOWNERS` file + branch protection = Sarah auto-assigned, required approval
- **Why:** Agents ship at 3 AM. Humans need automated routing.

### 3. Builds Are Deterministic
- **Wrong:** Tests pass locally, fail in CI, "works on my machine"
- **Right:** Hermetic builds, pinned dependencies, reproducible everywhere
- **Why:** Agents retry failed builds. Flaky tests = infinite loops.

### 4. CI Runs Only What's Affected
- **Wrong:** Change 1 file â†’ rebuild everything â†’ 4-hour feedback loop
- **Right:** Dependency graph analysis â†’ test only impacted projects â†’ 8-minute feedback
- **Why:** Slow CI = agents blocked. Blocked agents = humans blocked.

### 5. Repository Shape Is Consistent
- **Wrong:** Each team invents their own structure (17 different patterns)
- **Right:** Standardized `/apps`, `/libs`, `/platform` across all products
- **Why:** Agents navigate by pattern. Inconsistency = confusion = hallucinations.

> ğŸš¨ **Red Flag:** If our CI takes longer than our sprint retrospectives, agents can't help us.

---

## The Guardrails That Let Agents Run at Speed
*(Governance Primitives for Feature-Per-Day Delivery)*

### Auto-Routing with CODEOWNERS
```
/apps/payment-service/**  @payments-team
/libs/auth/**             @security-team
```
**Effect:** Agent opens PR â†’ GitHub auto-assigns â†’ No "who should review this?" Slack threads

### Branch Protection That Actually Protects
- âœ… Require 1 human approval from CODEOWNERS
- âœ… Require all CI checks green (tests, security scans, lint)
- âœ… Require up-to-date branch (no merge conflicts)
- âŒ Don't require 3 approvals (that's velocity poison)

### Policy-as-Code (Not Policy-as-PDF)
Encode in automated checks:
- ğŸ”’ **Security:** No secrets in code, no SQL injection patterns, no outdated dependencies
- ğŸ“œ **Compliance:** PII handling, data retention, audit trails
- âš–ï¸ **Licensing:** No GPL in proprietary code
- ğŸ¯ **Quality:** 80% test coverage, complexity < 15, no TODO comments in shipped code

### Agents Produce Evidence, Humans Validate Outcomes
**Agent generates:**
- âœ… Unit tests (327 assertions, 89% coverage)
- âœ… Integration tests (12 API contracts validated)
- âœ… Security scan (0 critical, 2 medium addressed)
- âœ… Risk: Low (no schema changes, no auth modifications)
- âœ… Rollback plan: Revert commit abc123, no data migration needed

**Human reviews:** "Outcomes match intent? Ship it."

> ğŸ’¡ **The New Contract:** Agents prove it's safe. Humans decide if it's right.

---

## CI as Your Assembly Line: No Flaky Bolts Allowed

### Affected Execution (Build Only What Changed)
```bash
# Wrong: Change 1 file, rebuild 47 projects
$ npm run build  # 4 hours ğŸŒ

# Right: Build only affected projects
$ nx affected:build  # 8 minutes âš¡
```
**Why:** Agents iterate fast. Slow CI = bottleneck.

### Remote Caching (Never Rebuild the Same Thing Twice)
- Agent A builds `@libs/auth` â†’ uploads to cache
- Agent B needs `@libs/auth` â†’ downloads from cache (3 seconds vs 4 minutes)
- Local dev needs `@libs/auth` â†’ same cache (our laptops thank us)

### Dependency-Graph-Aware Tooling
```
@libs/validation changed
  â†“ affects
@libs/api-client (needs rebuild)
  â†“ affects
@apps/web-app (needs tests)
  â†“ affects
@apps/mobile-app (unaffected, skipped)
```

### Flaky Tests = Production Incidents
**Treat them the same way:**
- ğŸš¨ Page the owning team
- ğŸ”¥ Fix within 24 hours or disable
- ğŸ“Š Track flake rate as a key metric

**Why:** One flaky test = agent retries = CI queue blocked = 12 other agents waiting = $3,000 of wasted compute per day

> âš¡ **If our CI fails 1 in 10 times for no reason, we don't have CIâ€”we have a coin flip.**

---

## PRs That Survive Featureâ€‘Perâ€‘Day Delivery

### What a Good PR Looks Like
```markdown
## Intent
Add character bio expansion on hover (FAN-472)

## Changes
- New `CharacterBioPopover` component (127 lines)
- Hook into existing `CharacterCard` click handler
- 15 unit tests, 3 integration tests

## Evidence
âœ… All tests pass (332/332)
âœ… Coverage: 91% (+3%)
âœ… Security scan: 0 issues
âœ… Bundle size: +2.3kb (within budget)
âœ… Lighthouse: 94 performance (no regression)

## Risk: Low
- No schema changes
- No auth modifications
- Feature flag: `character-bio-hover` (off by default)

## Rollback
Revert commit f3a90bc, toggle flag off
```

### What Humans Actually Review
1. **Does the implementation match the intent?** (Yes/No)
2. **Are the automated checks sufficient?** (Coverage looks good / Need more edge case tests)
3. **What's the blast radius if this breaks?** (5,000 users / 500,000 users)
4. **Should we increase autonomy for this agent?** (Yes, consistent quality / No, needs more supervision)

**Not:** "Why did you use `useState` here instead of `useReducer`?" (That's what automation enforces)

> ğŸ“¦ **PRs are evidence bundles, not collaboration forums.**

---

## Where a Separate Controlâ€‘Plane Repo Pays Off
*(The Factory That Builds Factories)*

### What Goes in the Control Plane

**`platform-standards/` repo:**
```
/golden-workflows/
  â”œâ”€â”€ ci-pipeline.yml          # Reusable CI template
  â”œâ”€â”€ security-scan.yml        # Consistent scanning
  â””â”€â”€ deploy-production.yml    # Standardized deploys

/scaffolding/
  â”œâ”€â”€ new-service-template/    # "nx generate @company/service"
  â”œâ”€â”€ new-app-template/        # "nx generate @company/app"
  â””â”€â”€ new-library-template/    # "nx generate @company/lib"

/policies/
  â”œâ”€â”€ security-baseline.yml    # OPA policies
  â”œâ”€â”€ licensing-rules.yml      # Allowed licenses
  â””â”€â”€ quality-gates.yml        # Coverage, complexity thresholds

/platform-contracts/
  â”œâ”€â”€ auth-service.proto       # gRPC contracts
  â”œâ”€â”€ logging-sdk/             # Shared SDKs
  â””â”€â”€ error-handling/          # Standard patterns
```

### Why This Matters
- **New repo?** Clone template, run one script, have full CI in 10 minutes (not 3 days)
- **Policy update?** Change once in control plane, all repos get it via workflow import
- **Platform upgrade?** Update SDK version in one place, dependabot PRs everywhere

> ğŸ­ **Control plane = consistency at scale. Without it, every repo is a unique snowflake (and snowflakes melt).**

---

## Migration Approach (Minimal Drama)
*(How to Get There Without Breaking Production)*

### Phase 1: Standardize Structure (Weeks 1-2)
- Pick one repo as the reference
- Establish `/apps`, `/libs`, `/platform` structure
- Document patterns, create templates

### Phase 2: Extract Control Plane (Weeks 3-4)
- Create `platform-standards` repo
- Move CI templates, scaffolding, policies
- Update existing repos to consume templates

### Phase 3: Merge Tightly Coupled Repos (Weeks 5-8)
- Start with 2-3 repos that change together 90% of the time
- Use git subtree or monorepo migration tools
- Keep git history (bisect still works)

### Phase 4: Enforce Boundaries (Weeks 9-12)
- Add ESLint rules for module boundaries
- Implement CODEOWNERS
- Turn on branch protection

### Phase 5: Optimize CI (Ongoing)
- Add affected detection
- Set up remote caching
- Fix flaky tests as you find them

### Measure Success
- **Cycle time:** Feature start â†’ production (track weekly)
- **CI efficiency:** Total CI minutes / useful CI minutes
- **Failure rate:** Red builds / total builds (aim for <2%)
- **Rollback rate:** Rollbacks / deploys (aim for <1%)

> ğŸ“Š **If we're not measuring, we're just migrating and hoping.**

---

## ğŸš© Red Flags Our Repo Isn't Ready for Agents

### We Know We're in Trouble When...

- âŒ Our CI takes longer than our sprint retrospectives
- âŒ "It works on my machine" is said more than twice a week
- âŒ We have a Slack channel called `#build-is-broken`
- âŒ Our CODEOWNERS file was last updated in 2022
- âŒ Developers regularly commit directly to `main` to "save time"
- âŒ We have >5 repos but <5 people (coordination overhead exceeds team size)
- âŒ "Let me check which version of the library we're using" is a daily question
- âŒ Our test suite has >10% flaky tests and we're not fixing them
- âŒ Merging a PR requires pinging 4 people in Slack
- âŒ Our monorepo builds everything on every commit (and takes 3 hours)

### Common Failure Modes (And How to Avoid Them)

#### Failure Mode 1: The Monorepo Swamp
**Symptom:** Everything depends on everything, 4-hour builds, teams bypass CI

**Root Cause:** No enforced boundaries

**Fix:**
```typescript
// .eslintrc.js
rules: {
  '@nx/enforce-module-boundaries': [
    'error',
    {
      depConstraints: [
        {
          sourceTag: 'scope:apps',
          onlyDependOnLibsWithTags: ['scope:libs', 'scope:platform']
        },
        {
          sourceTag: 'scope:libs',
          onlyDependOnLibsWithTags: ['scope:libs', 'scope:platform']
        }
      ]
    }
  ]
}
```

#### Failure Mode 2: The Review Bottleneck
**Symptom:** PRs sit for days, "waiting for review" is the top blocker

**Root Cause:** Unclear ownership, too many required approvals

**Fix:**
- CODEOWNERS for automatic assignment
- 1 required approval (not 3)
- Automation proves safety, humans validate outcomes

#### Failure Mode 3: The Flaky Test Death Spiral
**Symptom:** Tests fail randomly, developers re-run CI until it passes, confidence collapses

**Root Cause:** Treating flaky tests as annoyances, not incidents

**Fix:**
- Track flake rate as a KPI
- Page owning team when flake rate >2%
- Disable flaky tests immediately, fix within 24 hours
- Never merge with flaky tests

#### Failure Mode 4: The Multi-Repo Coordination Hell
**Symptom:** Simple feature takes 3 days across 5 repos, 12 handoffs, 2 rollbacks

**Root Cause:** Repos organized by team, not by product

**Fix:**
- Merge repos that change together >50% of the time
- Use monorepo for product boundaries
- Reserve multi-repo for actual regulatory/access boundaries

> âš ï¸ **Remember:** Monorepos fail when everything depends on everything. Fix boundaries, not structure.

---

## What "Better Software Faster" Actually Looks Like
*(The Measurable Outcomes)*

### Before Agentic Delivery
- ğŸ“… Feature cycle time: 2-3 weeks
- ğŸŒ CI feedback: 45 minutes (with retries)
- ğŸ‘¥ Review bottleneck: 18-24 hour median wait
- ğŸ”„ Rollback rate: 8% of deploys
- ğŸ˜° Developer stress: "Did I break production?"

### After Optimization
- âš¡ Feature cycle time: Same-day ship for green builds
- ğŸš€ CI feedback: 8 minutes (affected tests + caching)
- ğŸ¤– Review bottleneck: Auto-assigned, 2-hour median (automated evidence does the work)
- âœ… Rollback rate: <1% (automation catches issues pre-merge)
- ğŸ˜Œ Developer focus: "What outcome do we want?" (not "Did I indent correctly?")

### The New Normal

**Agents iterate rapidly:**
- 10-15 feature PRs per day per team
- Each PR: 300-800 lines, fully tested, security-scanned
- Humans review intent and outcomes (not syntax)

**Inside deterministic rails:**
- CI is green or red (never "maybe")
- Policies are enforced by code (not Slack messages)
- Rollback is one click (and tracked as a metric)

**Trust scales through proof:**
- Week 1: Human reviews every line (agent needs training)
- Week 4: Human reviews outcomes (agent shows consistency)
- Week 12: Human reviews only high-risk changes (agent has earned autonomy)

> ğŸ¯ **The Goal:** Feature-per-day delivery where humans govern and agents executeâ€”sustainably, safely, at scale.

---

## The Bottom Line

**Our repo structure is our agent's world.**

If that world has:
- âœ… Clear boundaries
- âœ… Fast feedback loops
- âœ… Automated guardrails
- âœ… Deterministic outcomes

**Then agents can run fast.**

If it doesn't?

**They'll run fast anywayâ€”right off a cliff.**

> ğŸï¸ **Build roads for Formula 1 cars, not horses. Our agents are already here.**
